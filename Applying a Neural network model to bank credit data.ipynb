{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "#from statsmodels.api import datasets\n",
    "from sklearn import datasets ## Get dataset from sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sklm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 35)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "## We use a preprocessed credit dataset from a German bank\n",
    "\n",
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model coul to be problematic when there is significant class imbalance. So, we oversample the minority cases; bad credit customers.\n",
    "## We create a data set with balanced cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 35)\n",
      "(1300,)\n"
     ]
    }
   ],
   "source": [
    "temp_Labels = Labels[Labels == 1] \n",
    "temp_Features = Features[Labels == 1,:]\n",
    "temp_Features = np.concatenate((Features, temp_Features), axis = 0)\n",
    "temp_Labels = np.concatenate((Labels, temp_Labels), axis = 0) \n",
    "\n",
    "print(temp_Features.shape)\n",
    "print(temp_Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We do a 3-fold nnested cross validation to estimate the optimal hyperparameters and perform model selection for a neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define inside and outside fold objects. \n",
    "\n",
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=3, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=3, shuffle = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We estimate the best hyperparameters using 3 fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.999\n"
     ]
    }
   ],
   "source": [
    "## Define the dictionary for the grid search and the model object to search on\n",
    "param_grid = {#\"alpha\":[0.0000001,0.000001,0.00001], \n",
    "              #\"early_stopping\":[True, False], \n",
    "              \"beta_1\":[0.95,0.90,0.80], \n",
    "              \"beta_2\":[0.999,0.9,0.8]}\n",
    "\n",
    "## Define the Neural Network model\n",
    "nn_clf = MLPClassifier(hidden_layer_sizes = (100,100),\n",
    "                       max_iter=300)\n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "nr.seed(3456)\n",
    "nn_clf = ms.GridSearchCV(estimator = nn_clf, param_grid = param_grid, \n",
    "                      cv = inside, # Use the inside folds\n",
    "                      scoring = 'recall',\n",
    "                      return_train_score = True)\n",
    "\n",
    "nr.seed(6677)\n",
    "nn_clf.fit(temp_Features, temp_Labels)\n",
    "#print(nn_clf.best_estimator_.alpha)\n",
    "#print(nn_clf.best_estimator_.early_stopping)\n",
    "print(nn_clf.best_estimator_.beta_1)\n",
    "print(nn_clf.best_estimator_.beta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We next perform the outer cross validation of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance metric = 0.870\n",
      "SDT of the metric       = 0.007\n",
      "Outcomes by cv fold\n",
      "Fold  1    0.875\n",
      "Fold  2    0.874\n",
      "Fold  3    0.859\n"
     ]
    }
   ],
   "source": [
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(nn_clf, temp_Features, temp_Labels, \n",
    "                                 cv = outside) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now build and test a model using the estimated optimal hyperparameters.\n",
    "## The training data subset must have the minority case oversampled.so, we create training and testing dataset, with oversampled minority cases for the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])\n",
    "\n",
    "## Oversample the minority case for the training data\n",
    "y_temp = y_train[y_train == 1] \n",
    "X_temp = X_train[y_train == 1,:]\n",
    "X_train = np.concatenate((X_train, X_temp), axis = 0)\n",
    "y_train = np.concatenate((y_train, y_temp), axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a neural network model object using the estimated optimal model hyperparameters and then fits the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.8,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(1115)\n",
    "nn_mod = MLPClassifier(hidden_layer_sizes = (100,100), \n",
    "                       #alpha = nn_clf.best_estimator_.alpha, \n",
    "                       #early_stopping = nn_clf.best_estimator_.early_stopping, \n",
    "                       beta_1 = nn_clf.best_estimator_.beta_1, \n",
    "                       beta_2 = nn_clf.best_estimator_.beta_2,\n",
    "                       max_iter = 300)\n",
    "nn_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score and print evaluation metrics for the model, using the test data subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "Actual positive       172                40\n",
      "Actual negative        43                45\n",
      "\n",
      "Accuracy        0.72\n",
      "AUC             0.72\n",
      "Macro precision 0.66\n",
      "Macro recall    0.66\n",
      " \n",
      "           Positive      Negative\n",
      "Num case      212            88\n",
      "Precision    0.80          0.53\n",
      "Recall       0.81          0.51\n",
      "F1           0.81          0.52\n"
     ]
    }
   ],
   "source": [
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = nn_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
