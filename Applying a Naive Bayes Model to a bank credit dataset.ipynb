{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to use a preprocessed credit datasset from a German bank on its customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "#from statsmodels.api import datasets\n",
    "from sklearn import datasets ## Get dataset from sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sklm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a numpy array for the features and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 35)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We first define a 10-fold cross validation object. We then define a Gaussian naive Bayes model. Finally, we print results from the cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance metric = 0.691\n",
      "SDT of the metric       = 0.093\n",
      "Outcomes by cv fold\n",
      "Fold  1    0.720\n",
      "Fold  2    0.660\n",
      "Fold  3    0.770\n",
      "Fold  4    0.690\n",
      "Fold  5    0.750\n",
      "Fold  6    0.430\n",
      "Fold  7    0.690\n",
      "Fold  8    0.760\n",
      "Fold  9    0.710\n",
      "Fold 10    0.730\n"
     ]
    }
   ],
   "source": [
    "nr.seed(321)\n",
    "cv_folds = ms.KFold(n_splits=10, shuffle = True)\n",
    "    \n",
    "nr.seed(498)\n",
    "NB_credit = GaussianNB()\n",
    "cv_estimate = ms.cross_val_score(NB_credit, Features, Labels, \n",
    "                                 cv = cv_folds) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance metric = 0.679\n",
      "SDT of the metric       = 0.106\n",
      "Outcomes by cv fold\n",
      "Fold  1    0.670\n",
      "Fold  2    0.390\n",
      "Fold  3    0.680\n",
      "Fold  4    0.710\n",
      "Fold  5    0.760\n",
      "Fold  6    0.770\n",
      "Fold  7    0.620\n",
      "Fold  8    0.760\n",
      "Fold  9    0.730\n",
      "Fold 10    0.700\n"
     ]
    }
   ],
   "source": [
    "nr.seed(498)\n",
    "NB_credit = GaussianNB()\n",
    "cv_estimate = ms.cross_val_score(NB_credit, Features, Labels, \n",
    "                                 cv = 10) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now build and test a model using a single split of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Define a naive Bayes model object and then fit the model to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_credit_mod = GaussianNB() \n",
    "NB_credit_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we score and print evaluation metrics for the model using the test data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "Actual positive        30               182\n",
      "Actual negative         6                82\n",
      "\n",
      "Accuracy        0.37\n",
      "AUC             0.69\n",
      "Macro precision 0.57\n",
      "Macro recall    0.54\n",
      " \n",
      "           Positive      Negative\n",
      "Num case      212            88\n",
      "Precision    0.83          0.31\n",
      "Recall       0.14          0.93\n",
      "F1           0.24          0.47\n"
     ]
    }
   ],
   "source": [
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = NB_credit_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next we will use a Bernoulli naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To do ernoulli naive Bayes model the numeric features must be dropped from the array and are done below.\n",
    "\n",
    "Features = Features[:,4:]\n",
    "Features[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold nested cross validation is used to estimate the optimal hyperparameter and perform model selection for the naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define inside and outside fold objects of the Bernoulli naive Bayes model.\n",
    "\n",
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=10, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We estimate the best hyperparameters using 10 fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# In this case, a grid for one hyperparameter: alpha is the smoothing parameter to avoid zero probabilities.\n",
    "# The model is fit on the grid and the best estimated hyperparameters are printed.\n",
    "\n",
    "nr.seed(3456)\n",
    "## Define the dictionary for the grid search and the model object to search on\n",
    "param_grid = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "## Define the NB regression model\n",
    "NB_clf = BernoulliNB() \n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "clf = ms.GridSearchCV(estimator = NB_clf, param_grid = param_grid, \n",
    "                      cv = inside, # Use the inside folds\n",
    "                      scoring = 'roc_auc',\n",
    "                      return_train_score = True)\n",
    "clf.fit(Features, Labels)\n",
    "print(clf.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now perform the outer loop of the cross validation to estimate model performance with the optimal hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance metric = 0.754\n",
      "SDT of the metric       = 0.040\n",
      "Outcomes by cv fold\n",
      "Fold  1    0.735\n",
      "Fold  2    0.701\n",
      "Fold  3    0.733\n",
      "Fold  4    0.745\n",
      "Fold  5    0.771\n",
      "Fold  6    0.757\n",
      "Fold  7    0.762\n",
      "Fold  8    0.857\n",
      "Fold  9    0.765\n",
      "Fold 10    0.713\n"
     ]
    }
   ],
   "source": [
    "#NB_credit = BernoulliNB(alpha = clf.best_estimator_.alpha)\n",
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(clf, Features, Labels, \n",
    "                                 cv = outside) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and testing subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "Actual positive       178                34\n",
      "Actual negative        34                54\n",
      "\n",
      "Accuracy        0.77\n",
      "AUC             0.78\n",
      "Macro precision 0.73\n",
      "Macro recall    0.73\n",
      " \n",
      "           Positive      Negative\n",
      "Num case      212            88\n",
      "Precision    0.84          0.61\n",
      "Recall       0.84          0.61\n",
      "F1           0.84          0.61\n"
     ]
    }
   ],
   "source": [
    "## We finally fit and score the Bernoulli naive Bayes model and display the performance metrics. \n",
    "\n",
    "NB_credit_mod = BernoulliNB(alpha = clf.best_estimator_.alpha) \n",
    "NB_credit_mod.fit(X_train, y_train)\n",
    "probabilities = NB_credit_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This probability is invariably skewed toward the majority case. Since the bank cares more about the minority case, we now redefine the model object with prior probability of 0.6 for the minority case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "Actual positive       116                96\n",
      "Actual negative        17                71\n",
      "\n",
      "Accuracy        0.62\n",
      "AUC             0.78\n",
      "Macro precision 0.65\n",
      "Macro recall    0.68\n",
      " \n",
      "           Positive      Negative\n",
      "Num case      212            88\n",
      "Precision    0.87          0.43\n",
      "Recall       0.55          0.81\n",
      "F1           0.67          0.56\n"
     ]
    }
   ],
   "source": [
    "NB_credit_mod = BernoulliNB(alpha = clf.best_estimator_.alpha,\n",
    "                            class_prior = [0.4,0.6]) \n",
    "NB_credit_mod.fit(X_train, y_train)\n",
    "probabilities = NB_credit_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
